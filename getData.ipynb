{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd  # 数据存储\n",
    "import requests  # 网页内容获取\n",
    "import re  # 解析数据\n",
    "from lxml import etree  # 解析数据\n",
    "import random\n",
    "import time  # 反反爬\n",
    "from bs4 import BeautifulSoup\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T02:34:32.044626200Z",
     "start_time": "2023-08-25T02:34:31.428217500Z"
    }
   },
   "id": "bb8489c44cc61fc6"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-25T02:34:34.902164600Z",
     "start_time": "2023-08-25T02:34:34.896120Z"
    }
   },
   "outputs": [],
   "source": [
    "def sleep_milliseconds(milliseconds):\n",
    "    seconds = milliseconds / 1000.0\n",
    "    time.sleep(seconds)\n",
    "\n",
    "def get_user_agent():\n",
    "    \"\"\"随机生成一个浏览器用户信息\"\"\"\n",
    "\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',\n",
    "        'Opera/9.25 (Windows NT 5.1; U; en)',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',\n",
    "        'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',\n",
    "        'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',\n",
    "        'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',\n",
    "        'Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7',\n",
    "        'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0',\n",
    "    ]\n",
    "\n",
    "    agent = random.choice(user_agents)\n",
    "\n",
    "    return {\n",
    "        'User-Agent': agent\n",
    "    }\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    获取网页源码\n",
    "    url: 目标网页的地址\n",
    "    return:网页源码\n",
    "    \"\"\"\n",
    "    res = requests.get(url=url, headers=get_user_agent())\n",
    "    return res.text\n",
    "\n",
    "\n",
    "def get_url_by_html(res_html):\n",
    "    \"\"\"\n",
    "    获取源码中每个二手房详情页的url\n",
    "    res_html:网页源码\n",
    "    return:列表形式的30个二手房详情页的url\n",
    "    \"\"\"\n",
    "    re_f = '<a class=\"\" href=\"(.*?)\" target=\"_blank\"'\n",
    "    url_list = re.findall(re_f, res_html)\n",
    "    return url_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "def parse_html(res_html):\n",
    "    \"\"\"获取房屋的详细数据\"\"\"\n",
    "    soup = BeautifulSoup(res_html, 'html.parser')\n",
    "\n",
    "    # 获取房屋的标题\n",
    "    title = soup.select_one(\"div.sellDetailHeader h1\")['title']\n",
    "    \n",
    "    # 获取房屋的总价\n",
    "    # total_price = soup.select(\"div.overview div span\")[2].get_text()\n",
    "    total_price = soup.findAll(\"span\",attrs={'class':'total'})[0].get_text()\n",
    "\n",
    "    # 获取房屋的单价\n",
    "    price = soup.find(\"span\",attrs={'class':'unitPriceValue'}).get_text()[0:-4]\n",
    "    \n",
    "    # 获取房屋的地段\n",
    "    place=[a.get_text() for a in soup.select(\"div.areaName span a\")]\n",
    "    \n",
    "    place=list(filter(None, place))\n",
    "    # 获取房屋基本信息\n",
    "    ## 获取房屋基本信息的标题\n",
    "    lab = [span.get_text() for span in soup.select(\"div.base span\")]\n",
    "    ## 获取房屋基本信息的内容\n",
    "    val = [li.contents[-1].strip() for li in soup.select(\"div.base li\")]\n",
    "  \n",
    "    # 获取房源交易信息\n",
    "    ## 获取房源交易标题\n",
    "    key1 = [span.get_text().strip() for span in soup.select(\"div.transaction span:nth-of-type(1)\")] #获取第一个span标签的内容\n",
    "    ## 获取房源交易信息内容\n",
    "    trans = [span.get_text().strip() for span in soup.select(\"div.transaction span:nth-of-type(2)\")]\n",
    "\n",
    "    # 获取房源特色信息\n",
    "    ## 获取房源特色标题\n",
    "    key = [div.get_text().strip() for div in soup.select(\"div.baseattribute div.name\")]\n",
    "    ## 获取房源特色内容\n",
    "    val1 = [div.get_text().strip() for div in soup.select(\"div.baseattribute div.content\")]\n",
    "\n",
    "    # 返回包含上述信息的字典\n",
    "    return dict(zip(['标题', '总价格', '单价', '地段'] + lab + key1 + key, [title, total_price, price, place] + val + trans + val1))\n",
    "# lab:  ['房屋户型', '所在楼层', '建筑面积', '户型结构', '套内面积', '建筑类型', '房屋朝向', '建筑结构', '装修情况', '梯户比例', '供暖方式', '配备电梯']\n",
    "# key1:  ['挂牌时间', '交易权属', '上次交易', '房屋用途', '房屋年限', '产权所属', '抵押信息', '房本备件']\n",
    "# key:  ['核心卖点', '小区介绍', '周边配套', '交通出行']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T02:45:22.486393100Z",
     "start_time": "2023-08-25T02:45:22.469456700Z"
    }
   },
   "id": "4ae95afa5028f3ec"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'标题': '双榆树榆苑公寓满五唯一诚心出售',\n '总价格': '1050',\n '单价': '99151',\n '地段': ['海淀', '双榆树'],\n '房屋户型': '2室2厅1厨1卫',\n '所在楼层': '高楼层 (共10层)',\n '建筑面积': '105.9㎡',\n '户型结构': '平层',\n '套内面积': '暂无数据',\n '建筑类型': '板楼',\n '房屋朝向': '南 北',\n '建筑结构': '钢混结构',\n '装修情况': '简装',\n '梯户比例': '两梯三户',\n '供暖方式': '集中供暖',\n '配备电梯': '有',\n '挂牌时间': '2023-08-14',\n '交易权属': '央产房',\n '上次交易': '2001-12-04',\n '房屋用途': '普通住宅',\n '房屋年限': '满五年',\n '产权所属': '非共有',\n '抵押信息': '无抵押',\n '房本备件': '已上传房本照片',\n '核心卖点': '满五唯一公房，税费少，带电梯，大客厅',\n '小区介绍': '榆苑公寓小区一共4栋楼，1号楼在双榆树北路北侧，2-4在南侧，1、2号楼有电梯，本房在二号楼，远洋物业，物业管理好楼道干净整洁，是四个楼里最hao的一栋，建成年代1998-2002',\n '周边配套': '地理位置优越， 购物休闲有：沃尔玛、苏宁易购、当代商场，双安商场等。医院有：海淀医院、中关村医院、北医三院。小区交通便利，10号线知春里地铁站，知春东里公交车站，开车可直接上知春路，小区东侧中关村东路连接北三环和北四环',\n '交通出行': '出小区有地铁10号线知春里站约550米的距离（此数据来源于百度地图）； 公交车站有知春里东站（304路、386路、579路、601路、653路、671路、79路等）和白塔庵站（323路、660路、86路、特19路等）出行方便；'}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://bj.lianjia.com/ershoufang/haidian/pg1/'\n",
    "url_list = get_url_by_html(get_html(url))\n",
    "data=parse_html(get_html(url_list[0]))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T02:45:25.767485800Z",
     "start_time": "2023-08-25T02:45:24.659066300Z"
    }
   },
   "id": "22e5a055d7ceb540"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def myspider(qu, start_pg=1, end_pg=100, tag=''):\n",
    "    print('爬虫程序开始运行')\n",
    "    \"\"\"爬虫程序\n",
    "    qu: 传入要爬取的qu的拼音的列表\n",
    "    start_pg:开始的页码\n",
    "    end_pg:结束的页码\n",
    "    download_times:第几次下载\n",
    "    \"\"\"\n",
    "    # 数据储存的列表\n",
    "    data = []\n",
    "    for q in qu:\n",
    "\n",
    "        # 获取当前区的首页url\n",
    "        url = 'https://bj.lianjia.com/ershoufang/' + q + '/'\n",
    "       \n",
    "        # 文件保存路径\n",
    "        #filename_csv = 'SecondHand-' + q + str(download_times) + 'tims_download.csv'\n",
    "        filename_excel='SecondHand_House_{}.xlsx'.format(tag)\n",
    "    \n",
    "        print(filename_excel)\n",
    "\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "\n",
    "            # 获取每页的url\n",
    "            new_url = url + 'pg' + str(i) + '/'\n",
    "\n",
    "            # 获取当前页面包含的30个房屋详情页的url\n",
    "            url_list = get_url_by_html(get_html(new_url))\n",
    "\n",
    "            for l in range(len(url_list)):\n",
    "\n",
    "                # 反爬随机停止一段时间\n",
    "                a = random.randint(2, 5)\n",
    "                if l % a == 0:\n",
    "                    sleep_milliseconds(2 * random.random())\n",
    "\n",
    "                # 获取当前页面的源码\n",
    "                text = get_html(url_list[l])\n",
    "                # 获取当前页面的房屋信息\n",
    "                data.append(parse_html(text))\n",
    "\n",
    "                # 反爬随机停止一段时间\n",
    "                sleep_milliseconds(3 * random.random())  # random.random()随机生成0-1之间的小数\n",
    "                print('Getting {} item!!'.format(l + 1)) \n",
    "            print('-----Getting {} page!!-----'.format(i + 1))\n",
    "            # 反爬随机停止一段时间\n",
    "            sleep_milliseconds(5 * random.random())\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                # 每5页保存一次数据\n",
    "                pd.DataFrame(data).to_excel(filename_excel, encoding='GB18030')\n",
    "                print('The data from the first {} pages has been saved.'.format(i))\n",
    "        # 保存数据\n",
    "        #pd.DataFrame(data).to_csv(filename_csv, encoding='GB18030')\n",
    "        pd.DataFrame(data).to_excel(filename_excel, encoding='GB18030')\n",
    "        print('SecondHand-House'+q  + tag + '_download_finished!!')\n",
    "    print('爬虫程序运行结束')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T02:34:57.017532600Z",
     "start_time": "2023-08-25T02:34:56.986235800Z"
    }
   },
   "id": "90ee3d8498aec636"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_page=int(input('Please input the start page:'))\n",
    "end_page=int(input('Please input the end page:'))\n",
    "tag=input('Please input the download tag:')\n",
    "#通过分区下载处理 非登录情况下只能下载100页的情况\n",
    "myspider(['chaoyang','fengtai','daxing','tongzhou','changping'], start_page, end_page, tag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e8a33c2208bb392"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
